{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "428efed0-395e-4670-9d64-cb3536be1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "18975aaf-1995-4096-b841-4af9f512b82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 events saved to ../data/test/raw-events/gh_events_202491.json\n",
      "Sampled 100 events saved to ../data/test/raw-events/gh_events_202492.json\n",
      "Sampled 100 events saved to ../data/test/raw-events/gh_events_202493.json\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "input_file = \"../data/datasets/raw-data-v2/gh_events_202409.json\"\n",
    "output_files = [\n",
    "    \"../data/test/raw-events/gh_events_202491.json\",\n",
    "    \"../data/test/raw-events/gh_events_202492.json\",\n",
    "    \"../data/test/raw-events/gh_events_202493.json\"\n",
    "]\n",
    "\n",
    "# Number of samples per output file\n",
    "sample_size = 100\n",
    "\n",
    "def sample_first_events(input_file, sample_size, output_files):\n",
    "    with open(input_file, 'r') as infile:\n",
    "        # Load all events as a list of dictionaries\n",
    "        events = json.load(infile)\n",
    "        \n",
    "        # Split into chunks and save to separate files\n",
    "        for i, output_file in enumerate(output_files):\n",
    "            start_idx = i * sample_size\n",
    "            end_idx = start_idx + sample_size\n",
    "            sample_events = events[start_idx:end_idx]\n",
    "            \n",
    "            # Save the sampled events to the output file\n",
    "            with open(output_file, 'w') as outfile:\n",
    "                json.dump(sample_events, outfile, indent=2)\n",
    "                \n",
    "            print(f\"Sampled {len(sample_events)} events saved to {output_file}\")\n",
    "\n",
    "# Run the sampling\n",
    "sample_first_events(input_file, sample_size, output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "442f3073-af25-47ee-855c-4551cef8af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed events saved to ../data/test/processed-events/gh_events_202491.json\n",
      "Processed events saved to ../data/test/processed-events/gh_events_202492.json\n",
      "Processed events saved to ../data/test/processed-events/gh_events_202493.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class EventProcessor:\n",
    "    def __init__(self, orgs_to_remove, input_folder, output_folder):\n",
    "        self.orgs_to_remove = orgs_to_remove\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.processed_ids = set()  # Track event IDs across all files\n",
    "        self.pending_events = []     # Store end-of-file events for cross-file checks\n",
    "\n",
    "    def parse_time(self, timestamp):\n",
    "        \"\"\"Converts Unix timestamp (in milliseconds) to a datetime object.\"\"\"\n",
    "        return datetime.fromtimestamp(timestamp / 1000)\n",
    "\n",
    "    def calculate_time_diff(self, start, end):\n",
    "        \"\"\"Calculates the difference in seconds between two datetime objects.\"\"\"\n",
    "        return (end - start).total_seconds()\n",
    "\n",
    "    def is_within_2s_window(self, event1, event2):\n",
    "        \"\"\"Checks if event2 is within a 2-second window of event1.\"\"\"\n",
    "        time_diff = abs(self.calculate_time_diff(\n",
    "            self.parse_time(event1['created_at']),\n",
    "            self.parse_time(event2['created_at'])\n",
    "        ))\n",
    "        return time_diff <= 2\n",
    "\n",
    "    def should_keep_event(self, current_event, events, index):\n",
    "        actor_id = current_event['actor']['id']\n",
    "        repo_id = current_event['repo']['id']\n",
    "\n",
    "        # Check preceding events for redundant comments\n",
    "        for j in range(index - 1, -1, -1):\n",
    "            if not self.is_within_2s_window(current_event, events[j]):\n",
    "                break\n",
    "            if events[j]['type'] == \"PullRequestReviewCommentEvent\" and \\\n",
    "               events[j]['actor']['id'] == actor_id and events[j]['repo']['id'] == repo_id:\n",
    "                return False\n",
    "\n",
    "        # Check following events for redundant comments\n",
    "        for j in range(index + 1, len(events)):\n",
    "            if not self.is_within_2s_window(current_event, events[j]):\n",
    "                break\n",
    "            if events[j]['type'] == \"PullRequestReviewCommentEvent\" and \\\n",
    "               events[j]['actor']['id'] == actor_id and events[j]['repo']['id'] == repo_id:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def filter_redundant_review_events(self, events):\n",
    "        \"\"\"Filters out redundant PullRequestReviewEvent events.\"\"\"\n",
    "        filtered_events = []\n",
    "        combined_events = self.pending_events + events  # Include end-of-file events from previous file\n",
    "        self.pending_events = combined_events[-3:]      # Store the last 3 events for next file processing\n",
    "\n",
    "        for i, event in enumerate(combined_events):\n",
    "            if event['type'] == \"PullRequestReviewEvent\" and event['id'] not in self.processed_ids:\n",
    "                if self.should_keep_event(event, combined_events, i):\n",
    "                    if not (filtered_events and \n",
    "                            filtered_events[-1]['type'] == \"PullRequestReviewEvent\" and \n",
    "                            filtered_events[-1]['actor']['id'] == event['actor']['id'] and \n",
    "                            filtered_events[-1]['repo']['id'] == event['repo']['id'] and\n",
    "                            self.is_within_2s_window(filtered_events[-1], event)):\n",
    "                        filtered_events.append(event)\n",
    "                        self.processed_ids.add(event['id'])\n",
    "            elif event['type'] != \"PullRequestReviewEvent\" and event['id'] not in self.processed_ids:\n",
    "                # Keep non-PullRequestReviewEvent events\n",
    "                filtered_events.append(event)\n",
    "                self.processed_ids.add(event['id'])\n",
    "\n",
    "        return filtered_events\n",
    "\n",
    "    def remove_unwanted_orgs(self, events):\n",
    "        \"\"\"Filters out events belonging to unwanted organizations.\"\"\"\n",
    "        return [event for event in events if event.get('org', {}).get('login') not in self.orgs_to_remove]\n",
    "\n",
    "    def process_all_files(self):\n",
    "        \"\"\"Processes each file in the input folder and writes processed events to separate output files.\"\"\"\n",
    "        os.makedirs(self.output_folder, exist_ok=True)  # Ensure output folder exists\n",
    "\n",
    "        for filename in sorted(os.listdir(self.input_folder)):\n",
    "            if filename.startswith('gh_events') and filename.endswith('.json'):\n",
    "                with open(os.path.join(self.input_folder, filename), 'r') as f:\n",
    "                    events = json.load(f)\n",
    "\n",
    "                # Remove events from unwanted organizations\n",
    "                events = self.remove_unwanted_orgs(events)\n",
    "\n",
    "                # Filter redundant review events\n",
    "                events = self.filter_redundant_review_events(events)\n",
    "\n",
    "                # Save processed events to an individual output file\n",
    "                output_path = os.path.join(self.output_folder, filename)\n",
    "                with open(output_path, 'w') as out_file:\n",
    "                    json.dump(events, out_file, indent=2)\n",
    "                print(f\"Processed events saved to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = '../data/test/raw-events'\n",
    "output_folder = '../data/test/processed-events'\n",
    "orgs_to_remove = [\"conda-forge\", \"Bioconductor\", \"openjournals\"]\n",
    "\n",
    "processor = EventProcessor(orgs_to_remove, input_folder, output_folder)\n",
    "processor.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75926938-ec34-4534-9e26-c40801b25f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
